# Combine Scripts

This is where the combine jobs are created and submitted, and the plots are generated.  

## Combine workflow

The workflow looks like this:
* Move your sigfit, multipdf and datacard into this working dir. (otherwise your jobs will fail!)
Like this:

Make sure your copied files have exactly the names listed below: these are the names expected in the first section of the datacard, and if they do not match, your combine jobs will fail instantly.

```
cd Plots/FinalResults
cp ../../Signal/CMS-HGG_sigfit_<ext>_<proc>_<tagname>.root CMS-HGG_mva_13TeV_sigfit_<proc>_<tagname>.root #to do with each of your (nTags x nProcs) sig files 
cp ../../Background/CMS-HGG_multipdf_<ext>.root CMS-HGG_mva_13TeV_multipdf.root
cp ../../Datacard/Datacard_13TeV_<ext>.txt CMS-HGG_mva_13TeV_datacard.txt
```

* Use the template to determine what kind of jobs you wish to run. 
To do this, use this template. BE CAREFUL: This template produces UNBLINDED results. If you want to make blinded plots, only run the jobs marked "expected" 

```
combineHarvesterOptions13TeV_Template.dat
```

And replace the required arguments like `!EXT!` and `!INTLUMI!` byt the desired values. If you are using the pilot script (`runFinalFitsScripts.sh` from `flashggFinalFit` dir,  using option `--combineOnly` ), this is done automatically.

* Use the `combineHarvester.py` to generate and submit jobs to the batch using this syntax, specifying the queue you wish to use:

```
./combineHarvester.py -d combineHarvesterOptions13TeV_<extension>.dat -q 1nh --batch LSF --verbose
#or if you are running at IC:
./combineHarvester.py -d combineHarvesterOptions13TeV_<extension>.dat -q hepshort.q --batch IC --verbose
```

* Use the same script to hadd the output once jobs are done.

```
./combineHarvester.py --hadd combineJobs13TeV_<extension>
```

* Adapt the template for generating plots (this takes the output of your hadded combine jobs and makes them into the relevant plot: limit, p-value, mh scan, mu scan etc ...):

```
combinePlotsOptions_Template.dat
```

And replace the required arguments like `!EXT!` and `!INTLUMI!` byt the desired values. This is done automagically if using the pilot script.

* Use the `./makeCombinePlots.py` to make plots, for example.

```
./makeCombinePlots.py -d combinePlotsOptions.dat #if using the dat file 
./makeCombinePlots.py -f combineJobs13TeV/MuScan/MuScan.root --mu -t "#sqrt{s}\=13TeV L\=x fb^{-1}" -o mu #if using manually
```

Example output can be found here:
```
https://twiki.cern.ch/twiki/bin/view/CMS/FLASHggFramework#Combine
```


Congratulations, you have finished!

## How to make Weighted/Unweighted S/S+B plots

This is how create the plots in Fig 8,9,10 of HIG-PAS-16-020.
The first step is to do a regular MultiDimFit.
This is what will be used to plot the signal and background models per tag, and also to calculate the expected amount of signal and background, and use that to make the weighted plot.
You also need 1/2 sigma uncertainty bands on the background plot.
To do this, you need to produce a large number of toy datasets, which are thrown from the post-fit background. To run the toys, a helper script `s_sb_errorbands.sh` is provided, and this needs to be used before running the plotting script. The purpose of this script is to take the output from your first fit, throw a toy from that, fit it, and then take the output from that fit to perform a second toy throwing from the post-Fit.
This script needs to be run several hundred times with a different random seed in order to build up a realistic 1/2 sigma band. I found that a few hundred was already enough to get sensible results.
Here is how the helper script is used: 
```
./scripts/s_sb_errorbands.sh <input seed> <working dir> <best-fit mass from original fit> <best-fit signal strength from original fit> <number of toys to run>
```
It is recommended to submit jobs to the batch to get the few hundred toys made quickly:

`<input seed>` is just the starting point 
`<working dir>` is the full path to where you are expecting to make your plots. This argument is needed so that when you submit jobs to the batch, the script will move to the right directory before beginning processing.
`<best-fit mass from original fit>` eg 125.97 in the example below
`<best-fit signal strength from original fit>` eg 0.95 in the example below
`<number of toys to run>` each time the script is called, how many toys it should run. Minimum 1, but you can set this higher if you prefer to launch fewer jobs with more toys.
In addition, the input file you use should be named inputfile.root.

You can then safely run the actual plotting macro `plotweightedbands.cpp` which for historical reasons lives in the `Background` directory:

```
./Background/bin/plotweightedbands -i inputfile.root --name example --lumi 12.9
```

This script will automagically pick up the toys generated by `s_sb_errorbands.sh` so long as you call this plotting script in the same directory as the toys and input file are stored.  

An example is probably the best way to learn how to run this complicated workflow.

E.g. for a Profiled MH (floating Higgs Mass) version:
```
#first make the datacard with the profiled MH physics model
text2workspace.py CMS-HGG_mva_13TeV_datacard.txt -o CMS-HGG_mva_13TeV_datacardMuScanMHProf.root -P HiggsAnalysis.CombinedLimit.PhysicsModel:floatingHiggsMass  -m125.09 
# run combine first time
combine -m 125.09 -M MultiDimFit --saveWorkspace -n CMS-HGG_mva_13TeV_datacardMuScanMHProf CMS-HGG_mva_13TeV_datacardMuScanMHProf.root 
# run combine again, to get post-fit 
combine higgsCombineCMS-HGG_mva_13TeV_datacardMuScanMHProf.MultiDimFit.mH125.09.root -M MultiDimFit --cminDefaultMinimizerType Minuit2 --cminDefaultMinimizerAlgo migrad --algo=grid -P r --floatOtherPOIs=1 --points=1 --firstPoint=0 --lastPoint=0 -n MuScanMHProfJob0 --setPhysicsModelParameterRanges r=-3.00,3.00  -m 125.09 --squareDistPoi --snapshotName MultiDimFit
#make a working directory
mkdir s_sb_weighted
cd s_sb_weighted
#copy the root file you just made, and call it inputfile.root 
mv ../higgsCombineCMS-HGG_mva_13TeV_datacardMuScanMHProf.MultiDimFit.mH125.09.root inputfile.root
#now you need to run a bunch of toys which are used to construct the 1/2sigma uncertainty bands for the background. This can be done one toy at a time locally (slow):
../scripts/s_sb_errorbands.sh 1 $PWD 125.97 0.95 200
# or better, on the batch eg, at CERN (LSF):
p=0 ; while (( $p<20 )); do bsub -q 1nh $PWD/../scripts/s_sb_errorbands.sh $p $PWD 125.97 0.95 5 ; (( p=$p+1 )) ; done
#once your jobs are done you should have 200 toys in your s_sb_weighted directory:
../../../Background/bin/plotweightedbands -i inputfile.root --name example --lumi 12.9
#well done, you have produced you plots, one for each category and one for the weighted and unweighed sums !
```

## Nuisance impacts
The impacts plot is a useful tool for cross-checking the analysis and often used during (pre)approval. You can follow the documentation from the combine group [here](https://twiki.cern.ch/twiki/bin/view/CMS/HiggsWG/SWGuideNonStandardCombineUses#Nuisance_parameter_impacts) and use either the `impactsPerProc.sh` or `impactsTotal.sh` guiding scripts.

## Notes

Running Notes:
* Make sure your copied files have exactly the names listed below: these are the names expected in the first section of the datacard, and if they do not match, your combine jobs will fail instantly.
* We previously had some issues with expected values not being exactly one, which was due to the best-fit background indices not being picked in the S+B fit. You can get around this by first doing the following command 
```
combine CMS-HGG_mva_13TeV_datacard.txt -M MultiDimFit -t -1 --expectSignal=1 -m 125 --saveSpecifiedIndex pdfindex_UntaggedTag_0_13TeV,pdfindex_UntaggedTag_1_13TeV,pdfindex_UntaggedTag_2_13TeV,pdfindex_UntaggedTag_3_13TeV,pdfindex_VBFTag_0_13TeV,pdfindex_VBFTag_1_13TeV,pdfindex_VBFTag_2_13TeV,pdfindex_TTHHadronicTag_13TeV,pdfindex_TTHLeptonicTag_13TeV,pdfindex_ZHLeptonicTag_13TeV,pdfindex_WHLeptonicTag_13TeV,pdfindex_VHLeptonicLooseTag_13TeV,pdfindex_VHHadronicTag_13TeV,pdfindex_VHMetTag_13TeV
```
and then looking at the indices and manually adjusting their values in the `--setPhysicsModelParameter` options in `combineHarvesterOptions13TeV_Template.dat`. This should fix the problem, providing your subsequent scan is then fine enough to get exactly one. 
